\documentclass[10pt]{scrartcl}

\usepackage[margin=2.5cm]{geometry}
\usepackage{microtype}
\usepackage{abstract}
\usepackage[ngerman]{babel}
\usepackage{biblatex}
\addbibresource{quellen.bib}
\usepackage{hyperref}
\usepackage{siunitx}
\sisetup{locale=DE}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{parskip}

\newcommand{\filepath}[1]{\texttt{#1}}

%%%%%%%%%%%%%%% ABBILDUNGEN
    \usepackage{graphicx}
    \usepackage{float} % Figure "H" Option zur Positionierung
    \usepackage{caption} % Für custom captions
    \captionsetup{textfont={footnotesize}, labelfont={footnotesize, bf}, position=below, format=plain, skip=7pt}
    % \DeclareCaptionOption{parskip}[]{} % Option "parskip" lahmlegen, damit subfig nicht darüber stolpert \usepackage{subfig}
    %%%%%%%% TIKZ
        \usepackage{tikz}
        \usetikzlibrary{matrix, positioning, calc, %
        decorations.pathreplacing, calligraphy, % for curly braces
        plotmarks % für mehr plot marks
        }
        \usepackage{circuitikz} % Schaltplan,Schaltzeichen usw.
        \tikzset{big elko/.style={elko=#1, capacitors/width=0.3}}
        \usepackage{pgfplots} % Plots
        \pgfplotsset{compat=1.18} % Neuste Version nutzen
        \usepackage{adjustbox} % Für Scaling von tikz pictures
    
%%%%%%%%%%%%%%% PROGRAMMAUSSCHNITTE
    \usepackage[newfloat=true]{minted}
    \usepackage{xparse} % Für NewDocumentCommand und -Environment
    \usepackage{varwidth}
    \definecolor{bg}{rgb}{0.95,0.95,0.95}
    \SetupFloatingEnvironment{listing}{name=Programmausschnitt, listname=Programmausschnitte}
    % https://stackoverflow.com/a/1390520
    %                             #1     #2       #3 #4 #5
    \NewDocumentEnvironment{code}{O{0.7} O{julia}  m  m}{%
        \VerbatimEnvironment%
        \begin{listing}[hp]%
            \centering% also for centering
            \begin{varwidth}{#1\textwidth}%
                \begin{minted}[bgcolor=bg, linenos, breaklines,fontsize=\footnotesize]{#2}%
                    } {%
                \end{minted}%
            \end{varwidth}%
            \vspace{-1ex}%
            \caption{#3}%
            \label{#4}%
        \end{listing}%
    }
    \newcommand{\coderef}[1]{Programmausschnitt \ref{#1}}

\usepackage{csquotes}
\usepackage{fancyhdr} % Kopf- und Fußzeilen
% https://tex.stackexchange.com/a/313337
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\todo}{$\square$}
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}

\setlist[itemize]{noitemsep}
\setlist[enumerate]{noitemsep}

\usepackage{tabularray}

\newcommand*{\eng}[1]{\textit{#1}}
\newcommand*{\feng}[1]{\eng{#1}}
\newcommand{\lee}{Lee {\itshape et al.} (2022)}

\pgfplotsset{every axis/.append style={
        scaled y ticks = false, 
        scaled x ticks = false, 
        y tick label style={/pgf/number format/.cd, fixed, fixed zerofill,
                            int detect,1000 sep={\;},precision=3},
        x tick label style={/pgf/number format/.cd, fixed, fixed zerofill,
                            int detect, 1000 sep={},precision=3}
    }
}

\usepackage{fontspec}
\newfontfamily\sansemph{Latin Modern Sans} % Für Titel

\begin{document}
\pagenumbering{gobble}
\thispagestyle{empty}

\vspace*{10mm}
\begin{center}
    {\Huge \textbf{\sansemph Analyse der Optimierungsverfahren mechanischer neuronaler Netzwerke}}%
    % \\[2mm]
    % {\Large Sammlung, Verarbeitung und Analyse unserer Gehirnsignale} 
    \\[4mm]
    \includegraphics[width=0.8\textwidth]{bilder/test2.pdf} \\[4mm]
    {\huge \textbf{Alexander Reimer \qquad Matteo Friedrich}} \\[1em]
    {\LARGE {Gymnasium Eversten Oldenburg}} \\[1.2ex]
    {\LARGE {Betreuer: Herr Dr. Glade}}
\end{center}

\newpage

\tableofcontents

\section*{Zusammenfassung}

Wir wollen uns mit dem neuen, noch vergleichsweise wenig erforschten Bereich der \feng{mechanical neural networks}, kurz MNNs, beschäftigen.
MNNs sind programmierbare Materialien, welchen verschiedene Verhaltensweisen, wie zum Beispiel ein bestimmtes Verformungsverhalten, antrainiert werden können. Sie bestehen aus Massepunkten (genannt Neuronen), welche durch Federn miteinander verbunden werden. Ihr Verhalten ergibt sich durch die Steifheiten der Federn.
Die grundlegende Annahme von MNNs ist, dass diese Federkonstanten in zukünftigen Materialien einzeln angepasst werden können. 
In Analogie zu künstlichen neuronalen Netzwerken wäre es dann prinzipiell möglich, durch eine geeignet gewählte Konfiguration an Federkonstanten verschiedene Verhaltensweisen auf externe Kräfte anzutrainieren.
Während sich die bisherige Forschung auf die technische, physische Implementation dieser Netzwerke fokussiert hat, wollen wir das Trainingsverfahren optimieren.
Dazu haben wir bereits eine Simulation eines MNNs umgesetzt, die bisher verwendeten Algorithmen (evolutionäres Lernen und Pattern Search) selbst implementiert, sowie mit neuen Parametern ausprobiert und verglichen. 
Dafür haben wir uns jedoch auf die Anwendung dieser Algorithmen in Simulationsrechnungen beschränkt. Die Ergebnisse sollten dennoch einen guten Startpunkt für reale MNNs bieten. Der von uns entwickelte Code ist die erste öffentlich verfügbare Implementation eines MNNs.
Unsere Ergebnisse zeigen, dass MNNs mehrere komplexe Verhaltensweisen lernen können. Diese intelligenten Materialien %TODO
eröffnen vielfältige zukünftige technologische Anwendungsmöglichkeiten. % für die Materialforschung.


\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\rightmark}
\fancyhead[R]{JUGEND FORSCHT -- PROJEKTBERICHT}
\pagenumbering{roman}

\newpage

\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\rightmark}
\fancyhead[R]{JUGEND FORSCHT -- PROJEKTBERICHT}
\pagenumbering{arabic}

\input{chapters/Einleitung}
\input{chapters/Grundlagen}

% \section{Forscherfrage}

% Ziel des Projektes ist es, ein neues Optimierungsverfahren für das Trainieren von MNNs zu entwickeln, sowie dieses mit den beiden im Artikel verwendeten Verfahren -- ein evolutionärer Algorithmus und Partial Pattern Search -- zu vergleichen.

% Ziel des Projektes ist es nicht, ein MNN selbst mechanisch umzusetzen.
% Es wird davon ausgegangen, dass 
% \begin{enumerate}
%     \item ein Vergleich der Verfahren in einer Simulation auch für das physische Trainieren der mechanischen Netzwerke aussagekräftig sein könnte und
%     \item ein \enquote{Vortrainieren} eines physischen MNN mit einer Simulation die Gesamtzeit zum Trainieren verkürzt und somit Optimierungsverfahren, auch wenn sie nur in einer Simulation verwendet werden, von Nutzen sind.
% \end{enumerate}

\newpage

\section{Vorgehensweise}

\subsection{Materialien}
\begin{itemize}
    \item Laptop (i7-11800H, 16\,GB, RTX 3060)
    \item Julia
    \begin{itemize}
        \item DifferentialEquations.jl und LinearAlgebra.jl für Autodifferenzierung der Simulation des Federsystems
        \item Graphs.jl und MetaGraphsNext.jl für Modellierung des Netzwerks als Graph
        \item Plots.jl, GLMakie.jl und Observables.jl für Visualisierung
        \item CSV.jl und Statistics.jl für Speicherung und Verarbeitung von Versuchsergebnissen
    \end{itemize}
\end{itemize}

\input{chapters/Vorgehensweise}


\newpage

\section{Ergebnisse}

Wie man in Abb. \ref{fig:ppsepochs} und Abb. \ref{fig:succes} sehen kann, kann man ein MNN mithilfe von PPS erfolgreich trainieren. Dabei fällt der MSE immer langsamer und kann nur noch durch viel Rechenzeit signifikant weiter gesenkt werden. Diese 500 Epochen, welche mit 14 verschiedenen Konstellationen von jeweils 3 verschiedenen Verhaltensweisen trainiert wurden, dauerten auf unserer Hardware ungefähr 14 Minuten. Das heißt, dass wir pro Verhaltensweise und Epoche ungefähr 0,04 Sekunden benötigen.

\begin{figure}[H]
    \centering
    \pgfplotstableread[col sep=comma]{bilder/PPSEpochs.csv}\Data
    \begin{tikzpicture}
        \begin{axis}[%xtick=data, 
        yticklabel style={
            /pgf/number format/precision=2,
            /pgf/number format/fixed,
        },
        xlabel={Epochen},
        ylabel={MSE},
        width=0.9\textwidth,
        height=6cm,
        xmin=5,
        xmax=505,
        title={MSE-Verlauf von PPS}
        ]
        \addplot [only marks]
            plot [mark size=1pt]
            table [x=epochs, y=loss_mean] {\Data};
        \end{axis}
    \end{tikzpicture}
    \caption{Die Punkte stellen den Mittelwert des MSE von 14 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter sind identisch. Originaldaten sind im Repository \cite{RepoMNN} in der Datei \filepath{src/data/PPSNumBehaviours\_2024-01-14T13:56:06.441.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num{0.968}$ und wurde für bessere y-Achsenskalierung entfernt.}
    \label{fig:ppsepochs}
\end{figure}

Das Training mit dem genetischen Algorithmus ist im Gegensatz zu PPS nicht erfolgreich gewesen.
Nach 200 Epochen lag der minimale MSE bei ungefähr \num{0.55} (s. Abb. \ref{fig:evolutionepochs}).
Dort befand er sich jedoch auch schon nach 80 Epochen, der MSE des Netzwerk befindet sich also in einem lokalem Minimum.
Mit der aktuellen hohen Lernrate von \num{0.05} werden keine feineren Anpassungen an den Federkonstanten mehr vorgenommen und ein signifikant niedriger MSE kann nicht erreicht werden.
Würde jedoch eine kleinere Lernrate gewählt werden, würden es sehr lange dauern, bis überhaupt erst dieser MSE von \num{0.55} erreicht wird.

\begin{figure}[H]
    \centering
    \pgfplotstableread[col sep=comma]{bilder/EvolutionEpochs.csv}\Data
    \begin{tikzpicture}
        \begin{axis}[%xtick=data, 
        yticklabel style={
            /pgf/number format/precision=2,
            /pgf/number format/fixed,
        },
        xlabel={Epochen},
        ylabel={MSE},
        width=0.9\textwidth,
        height=6cm,
        xmin=5,
        xmax=205,
        title={MSE-Verlauf des genetischen Algorithmus}
        ]
        \addplot [only marks]
            plot [mark size=1pt]
            table [x=epochs, y=loss_mean] {\Data};
        \end{axis}
    \end{tikzpicture}
    \caption{Die Punkte stellen den Mittelwert des MSE von 10 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit dem evolutionären Algorithmus trainiert und zur Simulation wurde das numerische Lösen genutzt, die restlichen Hyperparameter aller Netzwerke sind identisch zu den für Abb. \ref{fig:ppsepochs} verwendeten. 
    Originaldaten sind in der Datei \filepath{src/data/EvolutionEpochs\_2024-01-14T20:57:11.627.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num{0.968}$ und wurde für bessere y-Achsenskalierung entfernt.}
    \label{fig:evolutionepochs}
\end{figure}

Insgesamt lässt sich also sagen, dass in unserer Implementation PPS deutlich besser abschneidet als der evolutionäre Algorithmus, sowohl was Laufzeit als auch bisher erreichbare MSEs angeht.

Der einzige Vergleich von genetischer Optimierung und PPS, den wir finden konnten, war von \lee{}, deren Ergebnisse sich von unseren unterscheiden: Sie konnten mit dem genetischen Algorithmus einen deutlich niedrigeren MSE erreichen als mit PPS. Jedoch haben sie ersteren für mehr als 100 Stunden trainiert, und den zweiten weniger als drei Stunden \cite[Abb. 4]{Lee2022}; wie bei uns auch braucht der genetische Algorithmus deutlich länger, um die gleiche Leistung wie PPS zu erreichen.

Die Laufzeit scheint in der Tat bei PPS besser zu sein. Für eine Aussage über den niedrigsten erreichbaren MSE müssten jedoch das Problem des lokalen Minimums für den genetischen Algorithmus gelöst und beide sehr lange laufen gelassen werden. 

\begin{figure}[H]
  \centering
  \subfloat[Ein MNN, welches aus 5 Spalten und 3 Reihen besteht.]{\includegraphics[width=0.35\textwidth]{bilder/test.pdf}\label{fig:f1}}
  \hfill
  \subfloat[Ein MNN, welches aus 15 Spalten und 9 Reihen besteht.]{\scalebox{0.8}{\includegraphics[width=0.65\textwidth]{bilder/test2.pdf}\label{fig:f2}}}
  \caption{Die beiden MNNs wurden jeweils mit einer Verhaltensweise mit PPS trainiert. Die Kraftpfeile sind blau eingezeichnet und die roten Kreise geben an, wo sich die Ausgabeneuronen befinden sollen. Man erkennt, dass die Netzwerke erfolgreich trainiert wurden, das sich die Neuronen in den roten Kreisen befinden.}
  \label{fig:succes}
\end{figure}


\begin{figure}[H]
    \centering
    \pgfplotstableread[col sep=comma]{bilder/PPSNumBehavioursErrorBar.csv}\Data
    \begin{tikzpicture}
        \begin{axis}[xtick=data, error bars/y dir=both, error bars/y explicit, 
        yticklabel style={
            /pgf/number format/precision=2,
            /pgf/number format/fixed,
        },
        xlabel={Anzahl an Verhaltensweisen},
        ylabel={MSE},
        title={MSE von PPS, abhängig von Anzahl an Verhaltensweisen}
        ]
            \addplot [only marks]
                plot [mark size=1pt]
                table [x=behaviours, y=loss_median, y error =var] {\Data};
        \end{axis}
    \end{tikzpicture}
    \caption{Die Punkte stellen den Median des MSE von je 30-34 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter außer Anzahl der Verhaltensweise sind identisch. Originaldaten sind im Repository \cite{RepoMNN} im Ordner \filepath{src/data/}: \filepath{PPSNumBehaviours\_2024-01-14T15:16:06.598.csv}, \filepath{PPSNumBehaviours\_2024-01-14T15:08:40.927.csv} und \filepath{PPSNumBehaviours\_2024-01-14T13:56:06.441.csv}}
    \label{fig:numbehaviours}
\end{figure}

\begin{figure}[H]
    \centering
    \pgfplotstableread[col sep=comma]{bilder/EvolutionNumBehavioursMedian.csv}\Data
    \begin{tikzpicture}
        \begin{axis}[xtick=data, error bars/y dir=both, error bars/y explicit, 
        yticklabel style={
            /pgf/number format/precision=2,
            /pgf/number format/fixed,
        }, xlabel={Anzahl an Verhaltensweisen}, ylabel={MSE},
        title={MSE des genetischen Algorithmus, abhängig von Anzahl an Verhaltensweisen}
        ]
            \addplot [only marks]
                plot [mark size=1pt]
                table [x=behaviours, y=loss_median, y error=var] {\Data};
        \end{axis}
    \end{tikzpicture}
    \caption{Die Punkte stellen den Median des MSE von je 15 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit dem genetischen Algorithmus trainiert, die anderen Hyperparameter sind identisch zu Abb. \ref{fig:numbehaviours}. Originaldaten sind unter \filepath{src/data/EvolutionNumBehaviours\_2024-01-15T16:03:55.251.csv} zu finden.}
    \label{fig:numbehavioursevolution}
\end{figure}


\lee{} analysierten in ihrer Simulation unter anderem den Zusammenhang von MSE und Anzahl an Verhaltensweisen.
Es ist bei ihren Ergebnissen zu erkennen, dass der MSE tendenziell stark steigt, je mehr Verhaltensweisen gleichzeitig trainiert werden \cite[Abb. 5A u. 5C]{Lee2022}.
Wir konnten dieses Verhalten sowohl bei PPS als auch der evol. Opt. bestätigen (s. Abb. \ref{fig:numbehaviours} und \ref{fig:numbehavioursevolution})
% : bei PPS Bei fünf statt einer Verhaltensweise ist der MSE im Median ca. doppelt so hoch.
% Dies lässt sich durch die Erhöhung  

Weiter konnten wir den aus \cite[Abb. 5B]{Lee2022} ersichtlichen Zusammenhang bestätigen, zumindest beim Training mit PPS:
Je mehr Reihen das Netzwerk hat, desto höher der MSE, und je mehr Spalten das Netzwerk hat, desto niedriger der MSE (vgl. Abb. \ref{fig:ppsrowscols}).
Sowohl eine erhöhte Anzahl an Reihen als auch an Spalten erhöht die Menge an Federn und somit die notwendigen Schritte zur erfolgreichen Optimierung. Jedoch senkt eine höhere Anzahl an Spalten den MSE vermutlich dadurch, dass sie entlang der Kraftrichtung von links nach rechts liegen und so auch mehr Möglichkeiten zur Veränderung und Optimierung dieser bieten, während Reihen senkrecht zu dieser Richtung sind und ihre Anzahl zusätzlich proportional zur Anzahl an Kraftvektoren und Zielpositionen ist.

\begin{figure}[H]
    \centering
    \pgfplotstableread[col sep=comma]{bilder/PPSNumRowsColsMean.csv}\Data
    \begin{tikzpicture}
        \begin{axis}[
            view/h = 135,
            xlabel={Anzahl Reihen},
            x dir=reverse,
            zlabel={MSE},
            ylabel={Anzahl Spalten},
            title={MSE von PPS abhängig von Anzahl an Spalten und Reihen},
            ]
            \addplot3 [
            surf, shader=interp, mesh/cols=6,
            % mesh/ordering=y varies,
            ] table [x=rows, y=columns, z=loss_mean] {\Data};
        \end{axis}
    \end{tikzpicture}
    \caption{MSE in Abhängigkeit der Anzahl an Spalten und Reihen eines Netzwerks (niedrige MSE-Werte sind blau, hohe rot). Als Optimierungsalgorithmus wurde PPS verwendet. Pro Kombination gab es nur ein Netzwerk. Datei: \filepath{src/data/PPSNumRowsColumns\_2024-01-06T13:23:22.688.csv}}
    \label{fig:ppsrowscols}
\end{figure}

Unsere Annahme aus der Einleitung, dass wir auch durch unsere Simulation relevant Ergebnisse erzeugen können, scheint sich also zu bestätigen.
Denn wir kommen zu ähnlichen Schlüssen wie \lee{}, die ihren Simulationsalgorithmus und -ergebnisse anhand eines physischen Aufbaus bestätigen konnten.


% Eine andere Erkenntnis, welche wir durch unsere Analyse gewinnen konnten, ist das Verhältnis von dem MSE und der Anzahl der Trainingsdaten. Es lässt sich nämlich erkennen, dass es deutlich schwerer ist mehrere verschiedene Verhaltensweisen gleichzeitig zu trainieren, was in \ref{fig:numbehaviours} sehr deutlich erkennbar ist. Wenn wir mit nur 5 verschiedenen Verhaltensweisen trainieren, ist der MSE nach gleicher Anzahl von Epochen bereits ungefähr doppelt so groß, wie bei einer Verhaltensweise. Dies lässt sich durch die größere Komplexität erklären, welche durch jede neue Verhaltensweise hinzugefügt wird.


\subsection{Framework}

Die von uns geschriebene Bibliothek \cite{RepoMNN} soll nicht nur von uns leicht bedient und erweitert werden, sondern auch von anderen.
Hierzu fehlen zwar noch Dokumentation und Anleitungen, doch der Programmcode ist durch die Aufteilung der einzelnen Aufgabenbereiche und Aspekte der Bibliothek auf verschiedene Verbünde / Strukturen (s. Abb. \ref{fig:uml}) bereits entsprechend gestaltet (s. Prog. (Programmausschnitt) \ref{code:verwendung}).
Denn diese bieten gemeinsam mit den Möglichkeiten der Programmiersprache Julia, z.B.
%Julias Möglichkeiten 
der Funktionsüberladung die Option, einfach eigene Aspekte hinzuzufügen, ohne den Rest selbst schreiben zu müssen (s. Prog. \ref{code:anpassung}).

\begin{code}[1]{Beispielprogramm, welches MNN.jl verwendet}{code:verwendung}
using MNN
net = Network(5,4) # erstelle Netzwerk mit 5 Spalten und 4 Reihen

t1 = Trainer(net, PPS(), Diff(100)) # Training mit PPS und num. Simu. (100s Dauer)
t2 = Trainer(net, Evolution(), Diff(100)) # evol. Verfahren statt PPS
t3 = Trainer(net, Evolution(), Euler(100)) # Euler-Simulation statt numerisch

train!(net, 100, t1) # Training mit t1 für 100 Epochen
reset!(net) # Positionen der Neuronen zurücksetzen
# Netz + Verhaltensweise anzeigen; wird während Sim. autom. aktualisiert
vis = Visualizer(net, behaviour=t1.behaviours[1])
# 100s mit 1. Verhaltensweise simulieren
simulate!(net, Diff(100), t1.behaviours[1], vis = vis)
\end{code}

\begin{code}[1]{Verwendung von MNN.jl mit einer eigenen Simulation.
Diese kann ohne Veränderung des Quellcodes des Pakets hinzugefügt und nahtlos mit dem Rest des Pakets verwendet werden, indem der eigene Verbund als Untertyp von MNN.Simulation deklariert und die Funktion MNN.simulate! mit dem eignen Verbund überladen wird. 
Man muss also nicht noch Training, Visualisierung, MSE-Berechnung, Verhaltensweisenerstellung etc. selbst schreiben. Für das Hinzufügen eigener Trainingsverfahren und Visualisierungen wäre das Vorgehen ähnlich.
}{code:anpassung}
using MNN
mutable struct MySim <: MNN.Simulation
    modifier::Function
    # ...
end
# eigener Konstruktor; "modifier" wird später autom. von MNN.jl überschrieben
MySim(...) = MySim((net, acc) -> nothing), ...)

function MNN.simulate!(
    network::Network, sim::MySim, behaviour::Behaviour; 
    vis::Union{Visualizer,Nothing}=nothing
)
    # sim.modifier(net, acc), um Beschl.-vektor "acc" entspr. Kraftvektor anzupassen
    # MNN.update_positions!(vis, network), um Visualisierung zu aktualisieren
    # ...
end
# nutze MNN.jl wie üblich (s. vorheriger Programmausschnitt)
# ...
t = Trainer(net, PPS(), MySim(...))
# ...
simulate!(net, MySim(...), t1.behaviours[1], vis = vis)
# ...
\end{code}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{bilder/UML.pdf}
    \caption{UML-Diagramm des Programm-Aufbaus. Da es sich hier um Verbünde und nicht Klassen handelt, sind die Methoden nicht mit aufgelistet.}
    \label{fig:uml}
\end{figure}

Zudem ist die von uns entwickelte Bibliothek die erste Open Source-Implementation von MNNs und wir stellen auch die ersten öffentlichen Daten von den Trainingsverläufen für weitere Analysen zur Verfügung \cite{RepoMNN}.

\section{Diskussion}

Unser neuer Optimierungsansatz, Backpropagation, war zwar nicht erfolgreich, jedoch konnten wir erfolgreich die Leistung der zwei tatsächlich verwendeten Optimierungsalgorithmen beurteilen, auch in Abhängigkeit einiger Parameter -- sie stimmen mit dem aktuellen Forschungsstand überein.

Vor allem unsere Analyse der Abhängigkeit von Spaltenanzahl, Reihenanzahl und Anzahl an Verhaltensweisen ist interessant, da diese für MNNs bisher nur in Simulationen getestet wurden.
Jedoch wurde dafür z.B. bei \lee{} immer Gradient Descent verwendet \cite[6]{Lee2022Sup}, da dies schneller ist.
Wir haben diese Tests nun direkt mit den zu untersuchenden Verfahren durchgeführt, und konnten so verfahrensbedingte Unterschiede ausschließen.
Ausnahme bildet die Analyse der Auswirkungen der Spalten- und Reihenzahl, welche aufgrund der hohen Rechenzeit bisher nur für PPS durchgeführt wurde.
Den Test mit genetischer Optimierung wollen wir als nächstes durchführen, denn er könnte weniger unter einer erhöhten Reihenanzahl leiden, da anders als bei PPS \enquote{irrelevante} Federn die Trainingszeit vermutlich nicht erhöhen würden.

Weiter wollen wir das Problem des lokalen Minimums des genetischen Algorithmus lösen, um neben der Laufzeit nun auch bestimmen zu können, welche den niedrigsten MSE erreichen kann, unabhängig von Laufzeit.
Dies wäre vor allem interessant, da \lee{} durch die stark unterschiedliche Trainingszeiten der beiden Algorithmen (über 100 Stunden vs. unter 3 Stunden) noch keine ausreichenden Daten für diesen Vergleich liefern. Zum Lösen des Problems wollen wir die Lernrate des genetischen Algorithmus adaptiv machen, sodass sie zu Beginn des Training hoch ist, um den MSE schnell zu senken, und dann immer weiter sinkt, um lokalen Minima zu entkommen.

Da wir uns momentan auf den stabilen Endzustand der MNNs beschränken, könnte man die Differenzialgleichungen stark vereinfachen, indem man nicht die Beschleunigungen, sondern die Geschwindigkeiten der Neuronen mit den auf sie einwirkenden Kräften gleichsetzt.
Dadurch würde man Schwingungen in dem Netzwerk verhindern, da sich die Neuronen einfach direkt dem Ruhezustand annähern würden, wodurch die Lösung deutlich einfacher zu beschreiben ist und höchstwahrscheinlich auch schneller berechnet werden kann. 
Wenn man bedenkt, dass wir beim Trainieren der Netzwerke fast die ganze Zeit mit der Simulation der MNNs verbringen, könnte diese Veränderung eine große Verbesserung der Performance ausmachen und so mehr Vergleiche ermöglichen, vor allem rechenzeitintensive wie der im vorletzten Absatz erwähnte.

% Das Wählen der Hyperparameter ist eine zentrale Aufgabe bei der Optimierung von MNNs und kann höchstwahrscheinlich am besten durch eine adaptive Anpassung der Parameter während des Lernprozesses gelöst werden. 
% So kann bei einem großem MSE eine höhere Lernrate für größere Veränderungen der Federkonstanten und bei einem niedrigem MSE eine relativ niedrige Lernrate für feinere und genauere Anpassungen sorgen. Eine derartige adaptive Hyperparameteranspassung haben wir jedoch noch nicht entwickelt.

Es gibt es noch weitere Hyperparameter, die wir testen möchten sowie teilweise noch den CSV-Dateien als Spalten hinzufügen müssen, wie z.B. die Trainingsparameter für die Optimierungsverfahren (Startwert und Änderungsrate für PPS, \eng{population size} und Lernrate für genetische Optimierung).

Weiter wollen wir in der Zukunft Kombinationen an Optimierungsverfahren analysieren, vor allem die Anwendung von erst PPS und dann genetischen Algorithmen.
Denn so könnte man die Geschwindigkeit von PPS nutzen, um den MSE schnell zu senken, und nach Erreichen eines lokalen Minimums einen genetische Algorithmus, um einen niedrigeren MSE erreichen zu können -- vorausgesetzt, wir können bestätigen, dass letztere niedrigere MSEs als PPS erreichen können. 

Zusätzlich zu der bereits getesteten Architektur gibt es noch viele andere interessante Möglichkeiten, ein MNN aufzubauen.
Wenn man zum Beispiel versucht, erdbebensichere Häuser und einsturzsichere Brücken durch MNNs umzusetzen, müsste man das Resonanzverhalten analysieren und untersuchen, inwiefern ein MNN auf Schwingungen reagiert.
Vielleicht ist es sogar möglich, dem System eine genaue Resonanzkurve, die erlernt werden soll, als Trainingsdaten zu geben.
Dies könnte man umsetzen, indem man den Betrag der auf das MNN einwirkenden Kraft nicht konstant gleich lässt, sondern als Sinusfunktion der Zeit angibt.
Dadurch könnte man schwingende Kräfte simulieren, welche auch Schwingungen im Netzwerk auslösen würden.
Zudem müsste man auch die Verhaltensweisen ändern, da sie zusätzlich zu der Richtung der einwirkenden Kräfte auch noch die Frequenz der Schwingung einbeziehen müssten und zudem nicht die Positionen der Neuronen als zu lernendes Kriterium für die Verlustfunktion nutzen könnten.
Vielmehr müsste sich der Verlust darauf beziehen, wie stark das System, je nach Frequenz der Schwingungen der Kraft, selber anfängt zu schwingen.
Zum Beispiel wäre es vorstellbar, so einen Resonanzverstärker für Musikinstrumente zu bauen, welcher nur auf die Frequenzen reagiert, die zu den Tönen der Tonleiter gehören.
Aber auch Flugzeugflügel könnten als MNN gebaut werden, um auf die Turbulenzen der sie umströmenden Luft passsend zu reagieren.

Weiter sollten unsere Programme ebenso mit einem dreidimensionalen MNN funktionieren.
Die Implementation wäre nicht sehr aufwändig, da keine Änderung der Formeln und nur leichte Programmänderungen notwendig wären.
Durch die höhere Komplexität könnten mehr und komplexere Verhaltensweisen gleichzeitig erlernt werden, es würde aber der Trainingsaufwand voraussichtlich sehr stark steigen.
Bisher wurden dreidimensionale MNNs trotz der Vorteile (neue Anwendungszwecke, komplexere Verhaltensweisen) weder gebaut noch simuliert.
Wir planen, es zu probieren, sobald wir mit allen Optimierungsverfahren erfolgreich zweidimensionale Netzwerke trainieren können und ihre relevanten Hyperparameter analysiert haben.

Zusammenfassend stellen MNNs eine faszinierende Möglichkeit dar, Erkenntnisse der modernen KI mit Materialforschung zu verbinden. Dieses Forschungsfeld ist gerade erst im Entstehungsprozess. Wir sehen hier eine großes Zukunftspotenzial und wollen mit unserem Projekt einen Beitrag dazu leisten.

\newpage

\section{Quellen}

\printbibliography[heading=none]

\end{document}