\relax 
\providecommand*\new@tpo@label[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\abx@aux@refcontext{nty/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</P()>>}
\providecommand \oddpage@label [2]{}
\babel@aux{ngerman}{}
\HyPL@Entry{1<</S/r>>}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{Hopkins2023}
\abx@aux@segm{0}{0}{Hopkins2023}
\HyPL@Entry{2<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Einleitung}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mechanische Umsetzung eines MNN (von \blx@tocontentsinit {0}\cite {Lee2022})\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mnn2-1}{{1}{1}{Mechanische Umsetzung eines MNN (von \cite {Lee2022})\relax }{figure.caption.3}{}}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{wiki:hooke}
\abx@aux@segm{0}{0}{wiki:hooke}
\@writefile{toc}{\contentsline {section}{\numberline {2}Hintergrund und theoretische Grundlagen}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Ein Beispiel für ein neuronales Netzwerk bestehend aus einer Eingabeschicht($i_1$ und $i_2$) und zwei vollständig verbundene Schichten (Neuronen $h_1$ bis $h_4$ mit Bias $b_1$ und Neuron $O$ mit Bias $b_2$). Die Kreise stellen Neuronen dar; die Zahlen auf den Pfeilen die Gewichte zwischen den jeweiligen Neuronen.\relax }}{2}{figure.caption.4}\protected@file@percent }
\newlabel{fig:dense_nn}{{2}{2}{Ein Beispiel für ein neuronales Netzwerk bestehend aus einer Eingabeschicht($i_1$ und $i_2$) und zwei vollständig verbundene Schichten (Neuronen $h_1$ bis $h_4$ mit Bias $b_1$ und Neuron $O$ mit Bias $b_2$). Die Kreise stellen Neuronen dar; die Zahlen auf den Pfeilen die Gewichte zwischen den jeweiligen Neuronen.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Simulation von MNNs}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Skizze der verwendeten Federkraft $f(x)$ in Abhängigkeit der Auslenkung $x$ für den Fall einer positiven Federkonstante $k>0$ (links) und einer negativen Federkonstante $k<0$ (rechts). Die gestrichelte Linie zeigt einen linearen Kraftverlauf $f(x) = -kx$. Im Falle einer positiven Federkonstante (links) entspricht dies dem Hooke'schen Gesetz und führt zu einem stabilen Gleichgewicht, da eine positive Auslenkung der Feder zu einer negativen Kraft führt und umgedreht. Im Falle einer negativen Federkonstante ist das Gleichgewicht $x=0$ instabil und eine leichte Auslenkung der Feder führt zu einer unendlich großen Auslenkung. Um diese Instabilität zu vermeiden, verwenden wir die Kraft $f(x) = -kx - x^3$ (blaue durchgezogene Linie). Im Falle von $k>0$ führt dies zu einem leicht nichtlinearen Kraftverlauf, aber immer noch stabilem Gleichgewicht (links). Für $k<0$ ist das Gleichgewicht immer noch instabil, aber für große Auslenkungen $|x| > \sqrt  {|k|}$ wirkt die Kraft stabilisierend\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:force}{{3}{4}{Skizze der verwendeten Federkraft $f(x)$ in Abhängigkeit der Auslenkung $x$ für den Fall einer positiven Federkonstante $k>0$ (links) und einer negativen Federkonstante $k<0$ (rechts). Die gestrichelte Linie zeigt einen linearen Kraftverlauf $f(x) = -kx$. Im Falle einer positiven Federkonstante (links) entspricht dies dem Hooke'schen Gesetz und führt zu einem stabilen Gleichgewicht, da eine positive Auslenkung der Feder zu einer negativen Kraft führt und umgedreht. Im Falle einer negativen Federkonstante ist das Gleichgewicht $x=0$ instabil und eine leichte Auslenkung der Feder führt zu einer unendlich großen Auslenkung. Um diese Instabilität zu vermeiden, verwenden wir die Kraft $f(x) = -kx - x^3$ (blaue durchgezogene Linie). Im Falle von $k>0$ führt dies zu einem leicht nichtlinearen Kraftverlauf, aber immer noch stabilem Gleichgewicht (links). Für $k<0$ ist das Gleichgewicht immer noch instabil, aber für große Auslenkungen $|x| > \sqrt {|k|}$ wirkt die Kraft stabilisierend\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Typischer Aufbau eines MMNs. Dieses besteht aus Neuronen (schwarze Kreise), die durch Federn (schwarze Linien) verbunden sind. Die Neuronen am oberen und unteren Ende sind mechanisch fixiert und können sich nicht bewegen. Die blauen Pfeile stellen die Krafteinwirkung auf die Neuronen dar. \relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:positive_springs}{{4}{4}{Typischer Aufbau eines MMNs. Dieses besteht aus Neuronen (schwarze Kreise), die durch Federn (schwarze Linien) verbunden sind. Die Neuronen am oberen und unteren Ende sind mechanisch fixiert und können sich nicht bewegen. Die blauen Pfeile stellen die Krafteinwirkung auf die Neuronen dar. \relax }{figure.caption.6}{}}
\abx@aux@cite{0}{gentisch}
\abx@aux@segm{0}{0}{gentisch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Optimierungsverfahren von MNNs}{5}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Genetische Algorithmen}{5}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Partial Pattern Search}{5}{subsubsection.2.2.2}\protected@file@percent }
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\@writefile{toc}{\contentsline {section}{\numberline {3}Vorgehensweise}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Materialien}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Methoden}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Zwei Verhaltensweisen, die gemeinsam unmöglich erfolgreich anzutrainieren sind. Die schwarzen Punkte sind die Neuronen, die schwarzen Verbindungslinien die Federn. Die obersten und untersten zwei Neuronen sind jeweils fixiert. Bei beiden Verhaltensweisen sind die Kraftvektoren (blau) gleich, die Zielpositionen (rote Kreise) unterscheiden sich jedoch. Die Länge der Kraftvektoren wurde für bessere Erkennbarkeit verzehnfacht.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:impossibletraining}{{5}{7}{Zwei Verhaltensweisen, die gemeinsam unmöglich erfolgreich anzutrainieren sind. Die schwarzen Punkte sind die Neuronen, die schwarzen Verbindungslinien die Federn. Die obersten und untersten zwei Neuronen sind jeweils fixiert. Bei beiden Verhaltensweisen sind die Kraftvektoren (blau) gleich, die Zielpositionen (rote Kreise) unterscheiden sich jedoch. Die Länge der Kraftvektoren wurde für bessere Erkennbarkeit verzehnfacht.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Zwei Verhaltensweisen, deren gemeinsames Antrainieren möglich sein könnte, bei uns jedoch nicht vorkommen könnten, da mindestens ein Neuron der ersten Schicht (z.b. erstes von unten) in beiden Verhaltensweisen den gleichen Kraftvektor hat. Somit beträgt der Winkel zwischen diesen \ang {0} und ist entsprechend nie größer als der Mindestwinkel. Im Unterschied zu Abbildung \ref {fig:impossibletraining} wurde jedoch der Kraftvektor eines Neurons (rechtes Netzwerk, zweites von unten) verändert. Die Länge der Kraftvektoren wurde für bessere Erkennbarkeit verzehnfacht.\relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig:notimpossibletraining}{{6}{7}{Zwei Verhaltensweisen, deren gemeinsames Antrainieren möglich sein könnte, bei uns jedoch nicht vorkommen könnten, da mindestens ein Neuron der ersten Schicht (z.b. erstes von unten) in beiden Verhaltensweisen den gleichen Kraftvektor hat. Somit beträgt der Winkel zwischen diesen \ang {0} und ist entsprechend nie größer als der Mindestwinkel. Im Unterschied zu Abbildung \ref {fig:impossibletraining} wurde jedoch der Kraftvektor eines Neurons (rechtes Netzwerk, zweites von unten) verändert. Die Länge der Kraftvektoren wurde für bessere Erkennbarkeit verzehnfacht.\relax }{figure.caption.8}{}}
\abx@aux@cite{0}{brotcrunsher:backwardpass}
\abx@aux@segm{0}{0}{brotcrunsher:backwardpass}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Backpropagation}{8}{subsubsection.3.2.1}\protected@file@percent }
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\@writefile{toc}{\contentsline {section}{\numberline {4}Ergebnisse}{10}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Die Punkte stellen den Mittelwert des MSE von 14 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter sind identisch. Originaldaten sind im Repository \blx@tocontentsinit {0}\cite {RepoMNN} in der Datei \texttt  {src/data/PPSNumBehaviours\_2024-01-14T13:56:06.441.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num {0.968}$ und wurde für bessere y-Achsenskalierung entfernt.\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:ppsepochs}{{7}{10}{Die Punkte stellen den Mittelwert des MSE von 14 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter sind identisch. Originaldaten sind im Repository \cite {RepoMNN} in der Datei \filepath {src/data/PPSNumBehaviours\_2024-01-14T13:56:06.441.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num {0.968}$ und wurde für bessere y-Achsenskalierung entfernt.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Die Punkte stellen den Mittelwert des MSE von 10 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit dem evolutionären Algorithmus trainiert und zur Simulation wurde das numerische Lösen genutzt, die restlichen Hyperparameter aller Netzwerke sind identisch zu den für Abb. \ref {fig:ppsepochs} verwendeten. Originaldaten sind in der Datei \texttt  {src/data/EvolutionEpochs\_2024-01-14T20:57:11.627.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num {0.968}$ und wurde für bessere y-Achsenskalierung entfernt.\relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig:evolutionepochs}{{8}{10}{Die Punkte stellen den Mittelwert des MSE von 10 Netzwerken (alle verschiedene UUID) nach einer bestimmten Epochenzahl dar. Das Netzwerk wurde mit dem evolutionären Algorithmus trainiert und zur Simulation wurde das numerische Lösen genutzt, die restlichen Hyperparameter aller Netzwerke sind identisch zu den für Abb. \ref {fig:ppsepochs} verwendeten. Originaldaten sind in der Datei \filepath {src/data/EvolutionEpochs\_2024-01-14T20:57:11.627.csv} zu finden. Der Mittelwert des MSE vor dem Training (0 Epochen) ist $\approx \num {0.968}$ und wurde für bessere y-Achsenskalierung entfernt.\relax }{figure.caption.10}{}}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\newlabel{fig:f1}{{9a}{11}{Subfigure 9a}{subfigure.9.1}{}}
\newlabel{sub@fig:f1}{{(a)}{a}{Subfigure 9a\relax }{subfigure.9.1}{}}
\newlabel{fig:f2}{{9b}{11}{Subfigure 9b}{subfigure.9.2}{}}
\newlabel{sub@fig:f2}{{(b)}{b}{Subfigure 9b\relax }{subfigure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Die beiden MNNs wurden jeweils mit einer Verhaltensweise mit PPS trainiert. Die Kraftpfeile sind blau eingezeichnet und die roten Kreise geben an, wo sich die Ausgabeneuronen befinden sollen. Man erkennt, dass die Netzwerke erfolgreich trainiert wurden, das sich die Neuronen in den roten Kreisen befinden.\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:succes}{{9}{11}{Die beiden MNNs wurden jeweils mit einer Verhaltensweise mit PPS trainiert. Die Kraftpfeile sind blau eingezeichnet und die roten Kreise geben an, wo sich die Ausgabeneuronen befinden sollen. Man erkennt, dass die Netzwerke erfolgreich trainiert wurden, das sich die Neuronen in den roten Kreisen befinden.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ein MNN, welches aus 5 Spalten und 3 Reihen besteht.}}}{11}{subfigure.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Ein MNN, welches aus 15 Spalten und 9 Reihen besteht.}}}{11}{subfigure.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Die Punkte stellen den Median des MSE von je 30-34 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter außer Anzahl der Verhaltensweise sind identisch. Originaldaten sind im Repository \blx@tocontentsinit {0}\cite {RepoMNN} im Ordner \texttt  {src/data/}: \texttt  {PPSNumBehaviours\_2024-01-14T15:16:06.598.csv}, \texttt  {PPSNumBehaviours\_2024-01-14T15:08:40.927.csv} und \texttt  {PPSNumBehaviours\_2024-01-14T13:56:06.441.csv}\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:numbehaviours}{{10}{11}{Die Punkte stellen den Median des MSE von je 30-34 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit PPS trainiert und zur Simulation wurde das numerische Lösen genutzt, alle Hyperparameter außer Anzahl der Verhaltensweise sind identisch. Originaldaten sind im Repository \cite {RepoMNN} im Ordner \filepath {src/data/}: \filepath {PPSNumBehaviours\_2024-01-14T15:16:06.598.csv}, \filepath {PPSNumBehaviours\_2024-01-14T15:08:40.927.csv} und \filepath {PPSNumBehaviours\_2024-01-14T13:56:06.441.csv}\relax }{figure.caption.12}{}}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\abx@aux@cite{0}{Lee2022}
\abx@aux@segm{0}{0}{Lee2022}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Die Punkte stellen den Median des MSE von je 15 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit dem genetischen Algorithmus trainiert, die anderen Hyperparameter sind identisch zu Abb. \ref {fig:numbehaviours}. Originaldaten sind unter \texttt  {src/data/EvolutionNumBehaviours\_2024-01-15T16:03:55.251.csv} zu finden.\relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:numbehavioursevolution}{{11}{12}{Die Punkte stellen den Median des MSE von je 15 verschiedenen Durchläufen und die Fehlerbalken die Standardabweichung vom Median dar. Das Netzwerk wurde 100 Epochen mit dem genetischen Algorithmus trainiert, die anderen Hyperparameter sind identisch zu Abb. \ref {fig:numbehaviours}. Originaldaten sind unter \filepath {src/data/EvolutionNumBehaviours\_2024-01-15T16:03:55.251.csv} zu finden.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MSE in Abhängigkeit der Anzahl an Spalten und Reihen eines Netzwerks (niedrige MSE-Werte sind blau, hohe rot). Als Optimierungsalgorithmus wurde PPS verwendet. Pro Kombination gab es nur ein Netzwerk. Datei: \texttt  {src/data/PPSNumRowsColumns\_2024-01-06T13:23:22.688.csv}\relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig:ppsrowscols}{{12}{12}{MSE in Abhängigkeit der Anzahl an Spalten und Reihen eines Netzwerks (niedrige MSE-Werte sind blau, hohe rot). Als Optimierungsalgorithmus wurde PPS verwendet. Pro Kombination gab es nur ein Netzwerk. Datei: \filepath {src/data/PPSNumRowsColumns\_2024-01-06T13:23:22.688.csv}\relax }{figure.caption.14}{}}
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Framework}{13}{subsection.4.1}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {1}{\ignorespaces Beispielprogramm, welches MNN.jl verwendet\relax }}{13}{listing.caption.15}\protected@file@percent }
\newlabel{code:verwendung}{{1}{13}{Beispielprogramm, welches MNN.jl verwendet\relax }{listing.caption.15}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {2}{\ignorespaces Verwendung von MNN.jl mit einer eigenen Simulation. Diese kann ohne Veränderung des Quellcodes des Pakets hinzugefügt und nahtlos mit dem Rest des Pakets verwendet werden, indem der eigene Verbund als Untertyp von MNN.Simulation deklariert und die Funktion MNN.simulate! mit dem eignen Verbund überladen wird. Man muss also nicht noch Training, Visualisierung, MSE-Berechnung, Verhaltensweisenerstellung etc. selbst schreiben. Für das Hinzufügen eigener Trainingsverfahren und Visualisierungen wäre das Vorgehen ähnlich. \relax }}{13}{listing.caption.16}\protected@file@percent }
\newlabel{code:anpassung}{{2}{13}{Verwendung von MNN.jl mit einer eigenen Simulation. Diese kann ohne Veränderung des Quellcodes des Pakets hinzugefügt und nahtlos mit dem Rest des Pakets verwendet werden, indem der eigene Verbund als Untertyp von MNN.Simulation deklariert und die Funktion MNN.simulate! mit dem eignen Verbund überladen wird. Man muss also nicht noch Training, Visualisierung, MSE-Berechnung, Verhaltensweisenerstellung etc. selbst schreiben. Für das Hinzufügen eigener Trainingsverfahren und Visualisierungen wäre das Vorgehen ähnlich. \relax }{listing.caption.16}{}}
\abx@aux@cite{0}{RepoMNN}
\abx@aux@segm{0}{0}{RepoMNN}
\abx@aux@cite{0}{Lee2022Sup}
\abx@aux@segm{0}{0}{Lee2022Sup}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces UML-Diagramm des Programm-Aufbaus. Da es sich hier um Verbünde und nicht Klassen handelt, sind die Methoden nicht mit aufgelistet.\relax }}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig:uml}{{13}{14}{UML-Diagramm des Programm-Aufbaus. Da es sich hier um Verbünde und nicht Klassen handelt, sind die Methoden nicht mit aufgelistet.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Diskussion}{14}{section.5}\protected@file@percent }
\gdef\minted@oldcachelist{,
  default.pygstyle,
  AEFD8BE44B822F263CEEBDDFAE97B7774B70D487DDA73778A0B4926F5208F732.pygtex,
  79F01C40C09E60E7E7868412338E1D604B70D487DDA73778A0B4926F5208F732.pygtex}
\@writefile{toc}{\contentsline {section}{\numberline {6}Quellen}{16}{section.6}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{89AEB88CE665E006E2D19C2C21B2EFBB}
\abx@aux@defaultrefcontext{0}{brotcrunsher:backwardpass}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Hopkins2023}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Lee2022}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{Lee2022Sup}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{RepoMNN}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{gentisch}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{wiki:hooke}{nty/global//global/global}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{9.49994pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{16.77994pt}
\global\@namedef{scr@dte@subsubsection@lastmaxnumwidth}{24.55994pt}
\@writefile{toc}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\gdef \@abspage@last{18}
